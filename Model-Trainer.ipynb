{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "source": [
    "## BDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.0.1)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2 pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "import pandas as pd\n",
    "\n",
    "# Fonction pour extraire le texte d'un PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:  # V√©rifier si l'extraction a r√©ussi\n",
    "                text += page_text + \"\\n\"\n",
    "        return text.strip()  # Supprime les espaces inutiles\n",
    "    except Exception as e:\n",
    "        return f\"Error reading {pdf_path}: {str(e)}\"\n",
    "\n",
    "# Dossier contenant les PDF\n",
    "directory = \"BDD\"\n",
    "data = []\n",
    "\n",
    "# V√©rifier si le dossier existe\n",
    "if os.path.exists(directory):\n",
    "    # Parcourir tous les fichiers du dossier\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".pdf\"):  # V√©rifier que c'est un fichier PDF\n",
    "            pdf_path = os.path.join(directory, filename)\n",
    "            extracted_text = extract_text_from_pdf(pdf_path)\n",
    "            data.append([filename, extracted_text])  # Ajouter aux donn√©es\n",
    "\n",
    "    # Cr√©er un DataFrame\n",
    "    pdf_dataframe = pd.DataFrame(data, columns=[\"Title\", \"Content\"])\n",
    "\n",
    "    # Nettoyer les noms des fichiers\n",
    "    pdf_dataframe['Title'] = pdf_dataframe['Title'].str.replace(r'\\d+', '', regex=True)  # Supprimer les nombres\n",
    "    pdf_dataframe['Title'] = pdf_dataframe['Title'].str.replace(r'\\.pdf$', '', regex=True)  # Supprimer \".pdf\"\n",
    "    pdf_dataframe['Title'] = pdf_dataframe['Title'].str.replace(r'[_-]', ' ', regex=True)  # Remplacer \"_\" et \"-\" par des espaces\n",
    "\n",
    "else:\n",
    "    print(f\"Le dossier '{directory}' n'existe pas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs uniques du titre :\n",
      "['assignation  absenceNomDemandeur' 'assignation  absenceNomTribunal'\n",
      " 'requete  absenceNomOpposant' 'requete  absenceDateNaissanceDemandeur'\n",
      " 'requete  absenceNomDemandeur'\n",
      " 'assignation  absenceLieuNaissanceDemandeur'\n",
      " 'requete  absenceLieuNaissanceDemandeur' 'assignation  sansVices'\n",
      " 'requete  absenceDateDepot' 'requete  absenceNomChambre'\n",
      " 'assignation  absenceNomHuissier' 'assignation  absenceNomChambre'\n",
      " 'assignation  absenceNomAvocat' 'requete  absenceLieuR√©sidenceDemandeur'\n",
      " 'assignation  absenceDateHeureAudience' 'assignation  absenceSignature'\n",
      " 'requete  absenceProfessionDemandeur' 'assignation  absenceNomAssign√©'\n",
      " 'assignation  absenceDateNaissanceDemandeur' 'assignation  absenceAnn√©e'\n",
      " 'requete  absenceNomTribunal' 'requete  absenceLieuR√©sidenceOpposant'\n",
      " 'assignation  absenceDateAudience' 'requete  absenceSignature'\n",
      " 'assignation  absenceHeureAudience' 'requete  sansVices'\n",
      " 'assignation  absencedateAudience']\n"
     ]
    }
   ],
   "source": [
    "print(\"Valeurs uniques du titre :\")\n",
    "print(pdf_dataframe['Title'].unique()[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_j/41f8hxv911z0jxlfypdr3xrw0000gn/T/ipykernel_63564/1345455003.py:32: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  pdf_dataframe['Title'] = pdf_dataframe['Title'].replace(title_mapping)\n"
     ]
    }
   ],
   "source": [
    "title_mapping = {\n",
    "    'assignation  absenceNomDemandeur': 0,\n",
    "    'assignation  absenceNomTribunal': 1,\n",
    "    'requete  absenceNomOpposant': 2,\n",
    "    'requete  absenceDateNaissanceDemandeur': 3,\n",
    "    'requete  absenceNomDemandeur': 4,\n",
    "    'assignation  absenceLieuNaissanceDemandeur': 5,\n",
    "    'requete  absenceLieuNaissanceDemandeur': 6,\n",
    "    'assignation  sansVices': 7,\n",
    "    'requete  absenceDateDepot': 8,\n",
    "    'requete  absenceNomChambre': 9,\n",
    "    'assignation  absenceNomHuissier': 10,\n",
    "    'assignation  absenceNomChambre': 11,\n",
    "    'assignation  absenceNomAvocat': 12,\n",
    "    'requete  absenceLieuR√©sidenceDemandeur': 13,\n",
    "    'assignation  absenceDateHeureAudience': 14,\n",
    "    'assignation  absenceSignature': 15,\n",
    "    'requete  absenceProfessionDemandeur': 16,\n",
    "    'assignation  absenceNomAssign√©': 17,\n",
    "    'assignation  absenceDateNaissanceDemandeur': 18,\n",
    "    'assignation  absenceAnn√©e': 19,\n",
    "    'requete  absenceNomTribunal': 20,\n",
    "    'requete  absenceLieuR√©sidenceOpposant': 21,\n",
    "    'assignation  absenceDateAudience': 22,\n",
    "    'requete  absenceSignature': 23,\n",
    "    'assignation  absenceHeureAudience': 24,\n",
    "    'requete  sansVices': 25,\n",
    "    'assignation  absencedateAudience': 26\n",
    "}\n",
    "\n",
    "# Appliquer le mapping sur les titres\n",
    "pdf_dataframe['Title'] = pdf_dataframe['Title'].replace(title_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs uniques du titre :\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26]\n"
     ]
    }
   ],
   "source": [
    "print(\"Valeurs uniques du titre :\")\n",
    "print(pdf_dataframe['Title'].unique()[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution des classes avant sur√©chantillonnage :\n",
      "Title\n",
      "7     31\n",
      "0     10\n",
      "25    10\n",
      "8     10\n",
      "12     9\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "class_distribution = pdf_dataframe['Title'].value_counts()\n",
    "# Afficher uniquement les premi√®res valeurs de la distribution des classes\n",
    "print(\"Distribution des classes avant sur√©chantillonnage :\")\n",
    "print(class_distribution.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution des classes avant sur√©chantillonnage :\n",
      "Title\n",
      "7     31\n",
      "0     10\n",
      "25    10\n",
      "8     10\n",
      "12     9\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribution des classes apr√®s sur√©chantillonnage :\n",
      "Title\n",
      "2     32\n",
      "8     32\n",
      "15    32\n",
      "4     32\n",
      "11    32\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# V√©rifier la distribution des classes avant le sur√©chantillonnage\n",
    "class_distribution = pdf_dataframe['Title'].value_counts()\n",
    "print(\"Distribution des classes avant sur√©chantillonnage :\")\n",
    "print(class_distribution.head())\n",
    "\n",
    "# D√©finir le seuil minimal de valeurs par classe\n",
    "min_samples = 32\n",
    "\n",
    "# Liste pour stocker les nouvelles donn√©es apr√®s sur√©chantillonnage\n",
    "resampled_data = []\n",
    "\n",
    "for class_label, count in class_distribution.items():\n",
    "    class_df = pdf_dataframe[pdf_dataframe['Title'] == class_label]\n",
    "    \n",
    "    # Si la classe a moins de min_samples, on la sur√©chantillonne\n",
    "    if count < min_samples:\n",
    "        resampled_class_df = resample(class_df, replace=True, n_samples=min_samples, random_state=42)\n",
    "    else:\n",
    "        resampled_class_df = class_df  # Conserver la classe telle quelle si elle a au moins min_samples\n",
    "    \n",
    "    resampled_data.append(resampled_class_df)\n",
    "\n",
    "# Concat√©ner toutes les classes sur√©chantillonn√©es\n",
    "resampled_df = pd.concat(resampled_data, ignore_index=True)\n",
    "\n",
    "# M√©langer les donn√©es apr√®s le sur√©chantillonnage\n",
    "resampled_df = resampled_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# V√©rifier la distribution des classes apr√®s sur√©chantillonnage\n",
    "class_distribution_after = resampled_df['Title'].value_counts()\n",
    "print(\"\\nDistribution des classes apr√®s sur√©chantillonnage :\")\n",
    "print(class_distribution_after.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Legal BERT V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca416df034f463b96b1d1fe740ddc45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/435 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08cb3b79eb2f48fd9180cf91a9f3bfc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0882098600268364, 'eval_runtime': 17.327, 'eval_samples_per_second': 9.984, 'eval_steps_per_second': 1.27, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c900155d4bf54ceaa6a9f9a86ca75153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08567970246076584, 'eval_runtime': 14.0394, 'eval_samples_per_second': 12.322, 'eval_steps_per_second': 1.567, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e554c6f4c3fa492cb6d78d5d4189df89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08310308307409286, 'eval_runtime': 14.4375, 'eval_samples_per_second': 11.983, 'eval_steps_per_second': 1.524, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "609bb67af3ca4d228ae844c52439cdfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06717950105667114, 'eval_runtime': 15.1776, 'eval_samples_per_second': 11.398, 'eval_steps_per_second': 1.449, 'epoch': 4.0}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:603] . unexpected pos 774546176 vs 774546064",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/serialization.py:850\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 850\u001b[0m     \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/serialization.py:1114\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[0;32m-> 1114\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:783] . PytorchStreamWriter failed writing file data/71: file write failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 64\u001b[0m\n\u001b[1;32m     56\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     57\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel, \n\u001b[1;32m     58\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     59\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[1;32m     60\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtest_dataset\n\u001b[1;32m     61\u001b[0m )\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Entra√Æner le mod√®le\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# √âvaluer le mod√®le\u001b[39;00m\n\u001b[1;32m     67\u001b[0m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2165\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/trainer.py:2591\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2589\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2590\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2591\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\n\u001b[1;32m   2593\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2594\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2595\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/trainer.py:3056\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time)\u001b[0m\n\u001b[1;32m   3053\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save \u001b[38;5;241m=\u001b[39m is_new_best_metric\n\u001b[1;32m   3055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 3056\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3057\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/trainer.py:3192\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial)\u001b[0m\n\u001b[1;32m   3188\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_model(output_dir, _internal_call\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   3190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_only_model:\n\u001b[1;32m   3191\u001b[0m     \u001b[38;5;66;03m# Save optimizer and scheduler\u001b[39;00m\n\u001b[0;32m-> 3192\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_optimizer_and_scheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3193\u001b[0m     \u001b[38;5;66;03m# Save RNG state\u001b[39;00m\n\u001b[1;32m   3194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_rng_state(output_dir)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/trainer.py:3313\u001b[0m, in \u001b[0;36mTrainer._save_optimizer_and_scheduler\u001b[0;34m(self, output_dir)\u001b[0m\n\u001b[1;32m   3308\u001b[0m     save_fsdp_optimizer(\n\u001b[1;32m   3309\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfsdp_plugin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, output_dir\n\u001b[1;32m   3310\u001b[0m     )\n\u001b[1;32m   3311\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m   3312\u001b[0m     \u001b[38;5;66;03m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[39;00m\n\u001b[0;32m-> 3313\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOPTIMIZER_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3315\u001b[0m \u001b[38;5;66;03m# Save SCHEDULER & SCALER\u001b[39;00m\n\u001b[1;32m   3316\u001b[0m is_deepspeed_custom_scheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_deepspeed_enabled \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   3317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler, DeepSpeedSchedulerWrapper\n\u001b[1;32m   3318\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/serialization.py:849\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    846\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 849\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    850\u001b[0m         _save(\n\u001b[1;32m    851\u001b[0m             obj,\n\u001b[1;32m    852\u001b[0m             opened_zipfile,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    855\u001b[0m             _disable_byteorder_record,\n\u001b[1;32m    856\u001b[0m         )\n\u001b[1;32m    857\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/serialization.py:690\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 690\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    692\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:603] . unexpected pos 774546176 vs 774546064"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "# Charger le dataset\n",
    "df = resampled_df\n",
    "\n",
    "# Diviser les donn√©es en ensembles d'entra√Ænement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Content'], df['Title'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Charger le tokenizer de LegalBERT\n",
    "tokenizer = BertTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased')\n",
    "\n",
    "# Tokenisation des donn√©es\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(texts, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "train_encodings = tokenize_function(X_train.tolist())\n",
    "test_encodings = tokenize_function(X_test.tolist())\n",
    "\n",
    "# Cr√©er un Dataset personnalis√© pour BERT\n",
    "class LegalBERTDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels.iloc[idx])\n",
    "        return item\n",
    "\n",
    "# Cr√©er les datasets pour l'entra√Ænement et l'√©valuation\n",
    "train_dataset = LegalBERTDataset(train_encodings, y_train)\n",
    "test_dataset = LegalBERTDataset(test_encodings, y_test)\n",
    "\n",
    "# Charger le mod√®le pr√©-entra√Æn√© de LegalBERT pour la classification\n",
    "model = BertForSequenceClassification.from_pretrained('nlpaueb/legal-bert-base-uncased', num_labels=4)\n",
    "\n",
    "# Arguments d'entra√Ænement\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # R√©pertoire de sortie\n",
    "    evaluation_strategy=\"epoch\",     # Strat√©gie d'√©valuation par √©poque\n",
    "    learning_rate=2e-5,              # Taux d'apprentissage\n",
    "    per_device_train_batch_size=8,   # Taille du batch pour l'entra√Ænement\n",
    "    per_device_eval_batch_size=8,    # Taille du batch pour l'√©valuation\n",
    "    num_train_epochs=5,              # Nombre d'√©poques\n",
    "    weight_decay=0.01,               # D√©croissance du poids\n",
    "    logging_dir='./logs',            # R√©pertoire des logs\n",
    ")\n",
    "\n",
    "# Initialiser le Trainer\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "# Entra√Æner le mod√®le\n",
    "trainer.train()\n",
    "\n",
    "# √âvaluer le mod√®le\n",
    "trainer.evaluate()\n",
    "\n",
    "# Pr√©dictions sur le jeu de test\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# Afficher les r√©sultats des pr√©dictions\n",
    "print(predictions.predictions.argmax(axis=-1))\n",
    "\n",
    "# Sauvegarder le mod√®le et le tokenizer\n",
    "model.save_pretrained('./legal_bert_model')\n",
    "tokenizer.save_pretrained('./legal_bert_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemples de pr√©dictions compar√©es aux v√©ritables titres :\n",
      "   Title  Predicted_Label\n",
      "0      0                0\n",
      "1      1                1\n",
      "2      0                0\n",
      "3      2                2\n",
      "4      0                0\n",
      "Pr√©cision du mod√®le : 16.00%\n",
      "\n",
      "Matrice de confusion :\n",
      "[[10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  0  7  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 4  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [31  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  7  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 7  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 7  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 7  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  7  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 4  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 4  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]\n",
      " [ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]]\n",
      "\n",
      "Nombre de bonnes r√©ponses : 32\n",
      "Nombre de mauvaises r√©ponses : 168\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Charger le dataset\n",
    "df = pdf_dataframe\n",
    "\n",
    "# Charger le mod√®le et le tokenizer sauvegard√©s\n",
    "model = BertForSequenceClassification.from_pretrained('./legal_bert_model')\n",
    "tokenizer = BertTokenizer.from_pretrained('./legal_bert_model')\n",
    "\n",
    "# Fonction pour pr√©dire la classe d'un texte\n",
    "def predict(texts):\n",
    "    # Tokeniser les nouveaux textes\n",
    "    encodings = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    \n",
    "    # Obtenir les pr√©dictions du mod√®le\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encodings)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Obtenir les indices des classes pr√©dominantes\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    return predictions\n",
    "\n",
    "# Tester le mod√®le avec le contenu du dataset\n",
    "new_texts = df['Content'].tolist()  # Liste des contenus des PDFs\n",
    "predicted_labels = predict(new_texts)\n",
    "\n",
    "# Ajouter les pr√©dictions dans le dataframe\n",
    "df['Predicted_Label'] = predicted_labels.numpy()\n",
    "\n",
    "# Afficher les r√©sultats avec les titres\n",
    "print(\"Exemples de pr√©dictions compar√©es aux v√©ritables titres :\")\n",
    "print(df[['Title', 'Predicted_Label']].head(20))\n",
    "\n",
    "# Calcul de la pr√©cision\n",
    "accuracy = accuracy_score(df['Title'], df['Predicted_Label'])\n",
    "print(f\"Pr√©cision du mod√®le : {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Confusion Matrix pour visualiser les bonnes et mauvaises pr√©dictions\n",
    "conf_matrix = confusion_matrix(df['Title'], df['Predicted_Label'])\n",
    "print(\"\\nMatrice de confusion :\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Nombre de bonnes r√©ponses\n",
    "correct_predictions = (df['Title'] == df['Predicted_Label']).sum()\n",
    "print(f\"\\nNombre de bonnes r√©ponses : {correct_predictions}\")\n",
    "\n",
    "# Nombre de mauvaises r√©ponses\n",
    "incorrect_predictions = len(df) - correct_predictions\n",
    "print(f\"Nombre de mauvaises r√©ponses : {incorrect_predictions}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
